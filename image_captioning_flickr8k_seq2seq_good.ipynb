{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d01caba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrowdFlowerAnnotations.txt  Flickr8k.token.txt\t       machine_translation\r\n",
      "ExpertAnnotations.txt\t    Flickr_8k.devImages.txt    readme.txt\r\n",
      "Flicker8k_smaller\t    Flickr_8k.testImages.txt\r\n",
      "Flickr8k.lemma.token.txt    Flickr_8k.trainImages.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d7c78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 10:41:23.005711: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-19 10:41:25.662851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-19 10:41:25.662965: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-19 10:41:30.681672: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-19 10:41:30.681935: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-19 10:41:30.681964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1000268201_693b08cb0e': ['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .'], '1001773457_577c3a7d70': ['A black dog and a spotted dog are fighting', 'A black dog and a tri-colored dog playing with each other on the road .', 'A black dog and a white dog with brown spots are staring at each other in the street .', 'Two dogs of different breeds looking at each other on the road .', 'Two dogs on pavement moving toward each other .'], '1002674143_1b742ab4b8': ['A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .', 'A little girl is sitting in front of a large painted rainbow .', 'A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .', 'There is a girl with pigtails sitting in front of a rainbow painting .', 'Young girl with pigtails painting outside in the grass .']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "\n",
    "THE_SIZE = 72\n",
    "IMAGE_SIZE = (THE_SIZE, THE_SIZE)\n",
    "\n",
    "# Load the Flickr8k dataset\n",
    "images_dir = \"../../../data/Flicker8k_smaller/\"\n",
    "captions_file = \"../../../data/Flickr8k.token.txt\"\n",
    "\n",
    "captions = {}\n",
    "with open(captions_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        image_id, caption = line.strip().split(\"\\t\")[0], line.strip().split(\"\\t\")[1]\n",
    "        image_id = os.path.splitext(os.path.basename(image_id))[0]\n",
    "        if image_id not in captions:\n",
    "            captions[image_id] = []\n",
    "        captions[image_id].append(caption)\n",
    "\n",
    "\n",
    "first3pairs = {k: captions[k] for k in list(captions)[:3]}\n",
    "print(first3pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709b1b6",
   "metadata": {},
   "source": [
    "# Preprocess the images and captions\n",
    "\n",
    "## why `captions_list[:SAMPLE_DATA_COUNT]`\n",
    "* It will crash because of my Mac's capacity, take first 1000 sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a6dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 10:41:43.031273: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-19 10:41:43.031654: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-19 10:41:43.032031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (f3b674af7ced): /proc/driver/nvidia/version does not exist\n",
      "2023-03-19 10:41:43.047894: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A child in a pink dress is climbing up a set of stairs in an entry way .\n"
     ]
    }
   ],
   "source": [
    "def decode_and_resize(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "SAMPLE_DATA_COUNT = 750\n",
    "\n",
    "image_paths = sorted(os.listdir(images_dir))\n",
    "image_paths = [os.path.join(images_dir, path) for path in image_paths]\n",
    "\n",
    "images_list = [decode_and_resize(image_path) for image_path in image_paths]\n",
    "\n",
    "captions_list = [captions[os.path.splitext(os.path.basename(image_path))[0]] for image_path in image_paths]\n",
    "captions_text_all_in_one = [\"<start> \" + \" \".join(caption_list) + \" <end>\" for caption_list in captions_list]\n",
    "\n",
    "print(captions_list[0][0])\n",
    "\n",
    "captions_text = []\n",
    "for caption_list in captions_list[:SAMPLE_DATA_COUNT]:\n",
    "    captions_text.append(\"<start> \" + caption_list[0] + \" <end>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc6bee26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 72, 72, 3)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.array(images_list[:SAMPLE_DATA_COUNT])\n",
    "print(encoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faadcd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       "  'A girl going into a wooden building .',\n",
       "  'A little girl climbing into a wooden playhouse .',\n",
       "  'A little girl climbing the stairs to her playhouse .',\n",
       "  'A little girl in a pink dress going into a wooden cabin .'],\n",
       " ['A black dog and a spotted dog are fighting',\n",
       "  'A black dog and a tri-colored dog playing with each other on the road .',\n",
       "  'A black dog and a white dog with brown spots are staring at each other in the street .',\n",
       "  'Two dogs of different breeds looking at each other on the road .',\n",
       "  'Two dogs on pavement moving toward each other .'],\n",
       " ['A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .',\n",
       "  'A little girl is sitting in front of a large painted rainbow .',\n",
       "  'A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .',\n",
       "  'There is a girl with pigtails sitting in front of a rainbow painting .',\n",
       "  'Young girl with pigtails painting outside in the grass .']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ebea12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> A child in a pink dress is climbing up a set of stairs in an entry way . <end>',\n",
       " '<start> A black dog and a spotted dog are fighting <end>',\n",
       " '<start> A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl . <end>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17c03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 25\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\", oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(captions_text)\n",
    "\n",
    "captions_sequences = tokenizer.texts_to_sequences(captions_text)\n",
    "captions_padded = keras.preprocessing.sequence.pad_sequences(captions_sequences, maxlen=SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 #add 1 for <OOV>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "689c27cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   3    2   34 ...    0    0    0]\n",
      " [   3    2   16 ...    0    0    0]\n",
      " [   3    2   40 ...    0    0    0]\n",
      " ...\n",
      " [   3   16   61 ...    0    0    0]\n",
      " [   3    2 1270 ...    0    0    0]\n",
      " [   3    2   17 ...    0    0    0]]\n",
      "(750, 25)\n",
      "1275\n"
     ]
    }
   ],
   "source": [
    "print(captions_padded)\n",
    "print(captions_padded.shape)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d6f05",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc4a0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3   2  34   6   2  85 159  11  78  49   2 365  13 366   6  31 560 367\n",
      "    5   4   0   0   0   0]\n",
      " [  3   2  16  10   8   2 368  10  21 369   4   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  3   2  40  17 126   6 561  79   6  46  13   2 370 371  14  47 205   6\n",
      "    2 562   5   4   0   0]]\n",
      "[[  2  34   6   2  85 159  11  78  49   2 365  13 366   6  31 560 367   5\n",
      "    4   0   0   0   0   0]\n",
      " [  2  16  10   8   2 368  10  21 369   4   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  2  40  17 126   6 561  79   6  46  13   2 370 371  14  47 205   6   2\n",
      "  562   5   4   0   0   0]]\n",
      "(750, 24)\n",
      "(750, 24)\n"
     ]
    }
   ],
   "source": [
    "decoder_input_data = captions_padded[:, :-1]\n",
    "decoder_output_data = captions_padded[:, 1:]\n",
    "\n",
    "print(decoder_input_data[:3])\n",
    "print(decoder_output_data[:3])\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51776d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 72, 72, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 70, 70, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 35, 35, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 33, 33, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1048640   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,068,032\n",
      "Trainable params: 1,068,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "latent_dim = 64\n",
    "input_shape = (THE_SIZE, THE_SIZE, 3)\n",
    "\n",
    "encoder_inputs = keras.Input(shape=input_shape)\n",
    "x = layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(encoder_inputs)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "encoder_outputs = layers.Dense(latent_dim, activation=\"relu\")(x)\n",
    "\n",
    "encoder = keras.Model(inputs=encoder_inputs, outputs=encoder_outputs)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f1c0897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 24, 10)       12750       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 24, 64),     19200       ['embedding[1][0]',              \n",
      "                                 (None, 64),                      'input_4[0][0]',                \n",
      "                                 (None, 64)]                      'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 24, 1275)     82875       ['lstm[1][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 114,825\n",
      "Trainable params: 114,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "size_of_vector = 10\n",
    "\n",
    "decoder_inputs_layer = keras.Input(shape=(SEQ_LENGTH-1,))\n",
    "embedding_layer = layers.Embedding(vocab_size, size_of_vector, input_length=SEQ_LENGTH-1)(decoder_inputs_layer)\n",
    "\n",
    "# initial_h_state = Input(shape=(latent_dim,))\n",
    "# initial_c_state = Input(shape=(latent_dim,))\n",
    "# initial_state = [initial_h_state, initial_c_state]\n",
    "\n",
    "rnn_layer = layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "tmp, _, _ = rnn_layer(embedding_layer, initial_state=[encoder_outputs, encoder_outputs])\n",
    "decoder_dense = layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "decoder_outputs_layer = decoder_dense(tmp)\n",
    "decoder = keras.Model(inputs=[decoder_inputs_layer, encoder_outputs, encoder_outputs], outputs=decoder_outputs_layer)\n",
    "\n",
    "decoder.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e6909f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 72, 72, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 70, 70, 32)   896         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 35, 35, 32)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 33, 33, 64)   18496       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 16384)        0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 24, 10)       12750       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           1048640     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 24, 64),     19200       ['embedding[0][0]',              \n",
      "                                 (None, 64),                      'dense[0][0]',                  \n",
      "                                 (None, 64)]                      'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 24, 1275)     82875       ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,182,857\n",
      "Trainable params: 1,182,857\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Model([encoder_inputs, decoder_inputs_layer], decoder_outputs_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dce32a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 72, 72, 3)\n",
      "(750, 24)\n",
      "(750, 24)\n",
      "Epoch 1/50\n",
      "150/150 [==============================] - 13s 64ms/step - loss: 3.8200 - val_loss: 3.0090\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 2.8348 - val_loss: 2.6541\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 2.5974 - val_loss: 2.5195\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 13s 88ms/step - loss: 2.5005 - val_loss: 2.4681\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 11s 70ms/step - loss: 2.4365 - val_loss: 2.4005\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 10s 65ms/step - loss: 2.3636 - val_loss: 2.3321\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 2.2985 - val_loss: 2.2784\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 9s 59ms/step - loss: 2.2408 - val_loss: 2.2347\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 9s 59ms/step - loss: 2.1962 - val_loss: 2.1960\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 10s 63ms/step - loss: 2.1615 - val_loss: 2.1716\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 8s 55ms/step - loss: 2.1290 - val_loss: 2.1507\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 8s 56ms/step - loss: 2.1028 - val_loss: 2.1288\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 8s 56ms/step - loss: 2.0814 - val_loss: 2.1201\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 9s 59ms/step - loss: 2.0616 - val_loss: 2.1013\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 8s 56ms/step - loss: 2.0398 - val_loss: 2.0880\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 9s 58ms/step - loss: 2.0220 - val_loss: 2.0836\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 8s 56ms/step - loss: 2.0065 - val_loss: 2.0720\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 8s 54ms/step - loss: 1.9910 - val_loss: 2.0612\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 8s 53ms/step - loss: 1.9755 - val_loss: 2.0536\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 8s 55ms/step - loss: 1.9625 - val_loss: 2.0467\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 8s 52ms/step - loss: 1.9500 - val_loss: 2.0484\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 8s 52ms/step - loss: 1.9381 - val_loss: 2.0326\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 8s 55ms/step - loss: 1.9249 - val_loss: 2.0305\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 8s 52ms/step - loss: 1.9137 - val_loss: 2.0278\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 8s 51ms/step - loss: 1.9035 - val_loss: 2.0212\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 10s 69ms/step - loss: 1.8917 - val_loss: 2.0167\n",
      "Epoch 27/50\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.8810 - val_loss: 2.0113\n",
      "Epoch 28/50\n",
      "150/150 [==============================] - 10s 64ms/step - loss: 1.8711 - val_loss: 2.0118\n",
      "Epoch 29/50\n",
      "150/150 [==============================] - 11s 76ms/step - loss: 1.8611 - val_loss: 2.0078\n",
      "Epoch 30/50\n",
      "150/150 [==============================] - 9s 60ms/step - loss: 1.8522 - val_loss: 2.0038\n",
      "Epoch 31/50\n",
      "150/150 [==============================] - 9s 60ms/step - loss: 1.8426 - val_loss: 2.0040\n",
      "Epoch 32/50\n",
      "150/150 [==============================] - 8s 56ms/step - loss: 1.8345 - val_loss: 2.0032\n",
      "Epoch 33/50\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 1.8252 - val_loss: 1.9989\n",
      "Epoch 34/50\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.8163 - val_loss: 2.0033\n",
      "Epoch 35/50\n",
      "150/150 [==============================] - 8s 55ms/step - loss: 1.8103 - val_loss: 2.0003\n",
      "Epoch 36/50\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.8012 - val_loss: 1.9994\n",
      "Epoch 37/50\n",
      "150/150 [==============================] - 8s 52ms/step - loss: 1.7910 - val_loss: 2.0001\n",
      "Epoch 38/50\n",
      "150/150 [==============================] - 11s 73ms/step - loss: 1.7829 - val_loss: 1.9967\n",
      "Epoch 39/50\n",
      "150/150 [==============================] - 10s 66ms/step - loss: 1.7735 - val_loss: 1.9938\n",
      "Epoch 40/50\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.7632 - val_loss: 1.9916\n",
      "Epoch 41/50\n",
      "150/150 [==============================] - 10s 64ms/step - loss: 1.7550 - val_loss: 1.9928\n",
      "Epoch 42/50\n",
      "150/150 [==============================] - 8s 55ms/step - loss: 1.7462 - val_loss: 1.9907\n",
      "Epoch 43/50\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.7376 - val_loss: 1.9900\n",
      "Epoch 44/50\n",
      "150/150 [==============================] - 8s 54ms/step - loss: 1.7297 - val_loss: 1.9919\n",
      "Epoch 45/50\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 1.7222 - val_loss: 1.9921\n",
      "Epoch 46/50\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 1.7171 - val_loss: 1.9941\n",
      "Epoch 47/50\n",
      "150/150 [==============================] - 10s 68ms/step - loss: 1.7078 - val_loss: 1.9929\n",
      "Epoch 48/50\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 1.6981 - val_loss: 1.9914\n",
      "Epoch 49/50\n",
      "150/150 [==============================] - 10s 64ms/step - loss: 1.6916 - val_loss: 1.9926\n",
      "Epoch 50/50\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 1.6847 - val_loss: 1.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2fcd42eb50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "epochs = 50\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_output_data.shape)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], keras.utils.to_categorical(decoder_output_data, num_classes=vocab_size),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212446b4",
   "metadata": {},
   "source": [
    "# inference model\n",
    "* We can have another inference model\n",
    "* The key point is to \n",
    "  1. reuse `rnn_layer` from decoder part above\n",
    "  2. put `h_state` and `c_state` into `outputs`, otherwise the mode.predict() will not return those 2 hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6249e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 10)        12750       ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    multiple             19200       ['embedding_2[0][0]',            \n",
      "                                                                  'input_9[0][0]',                \n",
      "                                                                  'input_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1, 1275)      82875       ['lstm[3][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 114,825\n",
      "Trainable params: 114,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "the_inputs_layer = keras.Input(shape=(1,))\n",
    "the_embedding_layer = layers.Embedding(vocab_size, size_of_vector, input_length=SEQ_LENGTH-1)(the_inputs_layer)\n",
    "\n",
    "initial_h_state = keras.Input(shape=(latent_dim,))\n",
    "initial_c_state = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# the_rnn_layer = layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "tmp2, h_state, c_state = rnn_layer(the_embedding_layer, initial_state=[initial_h_state, initial_c_state])\n",
    "the_decoder_dense = layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "the_outputs_layer = the_decoder_dense(tmp2)\n",
    "inference_model = keras.Model(\n",
    "    inputs=[the_inputs_layer, initial_h_state, initial_c_state], \n",
    "    outputs=[the_outputs_layer, h_state, c_state])\n",
    "\n",
    "# inference_model.set_weights(decoder.get_weights())\n",
    "\n",
    "inference_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0034383f",
   "metadata": {},
   "source": [
    "# How to do inference \n",
    "1. encode input and retrieve initial decoder state\n",
    "  * Use the `encoder` to generate hidden state for decoder\n",
    "2. run one step of decoder with this initial state and a \"start of sequence\" token as target. Output will be the next target token\n",
    "3. Repeat with the current target token and current states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cdabab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 72, 3)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "(1, 64)\n",
      "(1, 64)\n",
      "(1, 64)\n",
      "[3]\n",
      "1/1 [==============================] - 1s 664ms/step\n",
      "(1, 64)\n",
      "(1, 64)\n",
      "[966]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "(1, 64)\n",
      "(1, 64)\n",
      "[350]\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "(1, 64)\n",
      "(1, 64)\n",
      "[350]\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "(1, 64)\n",
      "(1, 64)\n",
      "[350]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "<start> wrapping swims swims swims goats\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    print(input_seq.shape)\n",
    "    states_value = encoder.predict(np.array([input_seq]))\n",
    "    print(np.array(states_value).shape)\n",
    "\n",
    "\n",
    "    seed_text = \"<START>\"\n",
    "    seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    num_words = 5  \n",
    "    res = [seed_sequence[0]]\n",
    "    h_state = states_value\n",
    "    c_state = states_value\n",
    "    for i in range(num_words):\n",
    "        print(h_state.shape)\n",
    "        print(c_state.shape)\n",
    "        print(seed_sequence)\n",
    "\n",
    "        next_word_probs, h_state, c_state = inference_model.predict([np.array(seed_sequence), h_state, c_state])\n",
    "        # next_word_probs = inference_model.predict([np.array(seed_sequence), h_state, c_state])\n",
    "        \n",
    "        # lstm_layer = inference_model.layers[4] # assuming the LSTM layer is the third layer\n",
    "        # print(lstm_layer.states)\n",
    "        # print(inference_model.state_updates)\n",
    "        # h_state, c_state = lstm_layer.states\n",
    "        \n",
    "        # print(next_word_probs)\n",
    "\n",
    "        next_idx = np.argmax(next_word_probs[0][0])\n",
    "        # print(next_idx)\n",
    "\n",
    "        seed_sequence[0] = next_idx\n",
    "        res.append(next_idx)\n",
    "\n",
    "    generated_text = ' '.join([tokenizer.index_word[i] for i in res])\n",
    "    print(generated_text)\n",
    "\n",
    "\n",
    "decode_sequence(images_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050d00b",
   "metadata": {},
   "source": [
    "# inference randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b44b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 72, 3)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "(1, 64)\n",
      "[3]\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "0.9999999741849024\n",
      "[83]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1.0000000575382728\n",
      "[579]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1.0000000881846063\n",
      "[77]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1.0000000032305252\n",
      "[827]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1.000000074331183\n",
      "<start> face picks girls dusk airport\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def select_by_prob(probs):\n",
    "    print(sum(probs))\n",
    "    \n",
    "    total = 0.0\n",
    "    select = random.random()\n",
    "    for idx, prob in enumerate(probs):\n",
    "        total += prob\n",
    "        if total > select:\n",
    "            return idx\n",
    "            \n",
    "    return len(probs)-1\n",
    "    \n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    print(input_seq.shape)\n",
    "    states_value = encoder.predict(np.array([input_seq]))\n",
    "    print(np.array(states_value).shape)\n",
    "\n",
    "\n",
    "    seed_text = \"<START>\"\n",
    "    seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    num_words = 5  \n",
    "    res = [seed_sequence[0]]\n",
    "    h_state = states_value\n",
    "    c_state = states_value\n",
    "    for i in range(num_words):\n",
    "        # print(h_state.shape)\n",
    "        # print(c_state.shape)\n",
    "        print(seed_sequence)\n",
    "\n",
    "        next_word_probs, h_state, c_state = inference_model.predict([np.array(seed_sequence), h_state, c_state])\n",
    "        \n",
    "        # print(next_word_probs)\n",
    "\n",
    "        next_idx = select_by_prob(next_word_probs[0][0])\n",
    "        if next_idx == 0:\n",
    "            break\n",
    "        \n",
    "        seed_sequence[0] = next_idx\n",
    "        res.append(next_idx)\n",
    "\n",
    "    generated_text = ' '.join([tokenizer.index_word[i] for i in res])\n",
    "    print(generated_text)\n",
    "\n",
    "\n",
    "decode_sequence(images_list[0])\n",
    "    \n",
    "# seed_text = \"<START>\"\n",
    "# seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "# num_words = 5  \n",
    "# res = [seed_sequence[0]]\n",
    "# for i in range(num_words):\n",
    "#     print(seed_sequence)\n",
    "    \n",
    "#     next_word_probs = inference_model.predict(np.array(seed_sequence))\n",
    "#     # print(next_word_probs)\n",
    "    \n",
    "#     next_idx = select_by_prob(next_word_probs[0][0])\n",
    "#     # print(next_idx)\n",
    "#     if next_idx == 0:\n",
    "#         break\n",
    "    \n",
    "#     seed_sequence[0] = next_idx\n",
    "#     res.append(next_idx)\n",
    "    \n",
    "# generated_text = ' '.join([tokenizer.index_word[i] for i in res])\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef699f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
